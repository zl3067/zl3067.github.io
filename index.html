<html>

<head>
  <meta charset="utf-8">
  
  <title>Zijian Liu</title>

  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js">
  </script>

  <style>
    a {
      color: #007bff;
      text-decoration: none;
    }
  </style>
</head>

<body>

  <h1>Zijian Liu</h1>

  <p>Email: zl3067 at stern dot nyu dot edu </p>

  <p>[<a href="https://scholar.google.com/citations?user=SbaGY5cAAAAJ&hl=en">Google Scholar</a>] [<a href="CV.pdf">CV</a>] </p>

  <h2>About Me</h2>
  <p>
    I am currently a Ph.D. candidate majoring in Operations Management in the Department of Technology, Operations, and Statistics at the Leonard N. Stern School of Business, New York University. My advisor is Zhengyuan Zhou.
  </p>

  <h2>Research Interests</h2>

  <p>Mathematical Optimization, Theoretical Machine Learning</p>

  <h2>Research Work</h2>
  * indicates equal contribution, &dagger; indicates alphabetical order

  <h3>Publications</h3>

    <ol reversed>
      <li>
        <a href="https://arxiv.org/abs/2508.07473">Online Convex Optimization with Heavy Tails: Old Algorithms, New Regrets, and Applications</a><br/>
        Zijian Liu <br/>
        In Proceedings of the 37th International Conference on Algorithmic Learning Theory (ALT 2026) (To appear)
      </li>

      <li>
        <a href="https://openreview.net/forum?id=BdQKHcrBxT">Improved Last-Iterate Convergence of Shuffling Gradient Methods for Nonsmooth Convex Optimization</a><br/>
        Zijian Liu, Zhengyuan Zhou<br/>
        In Proceedings of the 42nd International Conference on Machine Learning (ICML 2025)
      </li>

      <li>
        <a href="https://openreview.net/forum?id=NKotdPUc3L">Nonconvex Stochastic Optimization under Heavy-Tailed Noises: Optimal Convergence without Gradient Clipping</a><br/>
        Zijian Liu, Zhengyuan Zhou<br/>
        In the 13th International Conference on Learning Representations (ICLR 2025)
      </li>

      <li>
        <a href="https://openreview.net/forum?id=Xdy9bjwHDu">On the Last-Iterate Convergence of Shuffling Gradient Methods</a><br/>
        Zijian Liu, Zhengyuan Zhou<br/>
        In Proceedings of the 41st International Conference on Machine Learning (ICML 2024) (<b style="color:red">Oral Presentation</b>)
      </li>

      <li>
        <a href="https://openreview.net/forum?id=VDgfJnOEMV">On the Convergence of Projected Bures-Wasserstein Gradient Descent under Euclidean Strong Convexity</a><br/>
        Junyi Fan*, Yuxuan Han*, Zijian Liu, Jian-Feng Cai, Yang Wang, Zhengyuan Zhou<br/>
        In Proceedings of the 41st International Conference on Machine Learning (ICML 2024)
      </li>

      <li>
        <a href="https://openreview.net/forum?id=xxaEhwC1I4">Revisiting the Last-Iterate Convergence of Stochastic Gradient Methods </a><br/>
        Zijian Liu, Zhengyuan Zhou<br/>
        In the 12th International Conference on Learning Representations (ICLR 2024)
      </li>

      <li>
        <a href="https://proceedings.mlr.press/v195/liu23c.html">Breaking the Lower Bound with (Little) Structure: Acceleration in Non-Convex Stochastic Optimization with Heavy-Tailed Noise</a><br/>
        Zijian Liu, Jiawei Zhang, Zhengyuan Zhou<br/>
        In Proceedings of the 36th Annual Conference on Learning Theory (COLT 2023)
      </li>

      <li>
        <a href="https://openreview.net/forum?id=Q9SeUwcdfQ">High Probability Convergence of Stochastic Gradient Methods</a><br/>
        Zijian Liu*, Ta Duy Nguyen*, Thien Hang Nguyen*, Alina Ene, Huy L. Nguyen<br/>
        In Proceedings of the 40th International Conference on Machine Learning (ICML 2023)
      </li>

      <li>
        <a href="https://openreview.net/forum?id=ULnHxczCBaE">On the Convergence of AdaGrad(Norm) on \(\mathbb{R}^{d}\): Beyond Convexity, Non-Asymptotic Rate and Acceleration</a><br/>
        Zijian Liu*, Ta Duy Nguyen*, Alina Ene, Huy L. Nguyen<br/>
        In the 11th International Conference on Learning Representations (ICLR 2023)
      </li>

      <li>
        <a href="https://proceedings.mlr.press/v162/liu22o.html">Adaptive Accelerated (Extra-)Gradient Methods with Variance Reduction</a><br/>
        Zijian Liu*, Ta Duy Nguyen*, Alina Ene, Huy L. Nguyen<br/>
        In Proceedings of the 39th International Conference on Machine Learning (ICML 2022)
      </li>

      <li>
        <a href="https://proceedings.mlr.press/v162/liu22a.html">Distributionally Robust \(Q\)-Learning</a><br/>
        Zijian Liu, Qinxun Bai, Jose Blanchet, Perry Dong, Wei Xu, Zhengqing Zhou, Zhengyuan Zhou<br/>
        In Proceedings of the 39th International Conference on Machine Learning (ICML 2022)
      </li>

    </ol>

   <h3>Manuscripts</h3>

   <ol reversed>

      <li>
        <b>Product Return as a Sequence of Search Processes: Optimality and Search Duration</b><br/>
        Xiaoyu Fan&dagger;, Srikanth Jagabathula&dagger;, Zijian Liu&dagger;, Eitan Muller&dagger;<br/>
        Major Revision at Operations Reseaerch, 2025 (<span style="color:blue">Available upon request</span>)
      </li>

      <li>
        <b>Clipped Gradient Methods for Nonsmooth Convex Optimization under Heavy-Tailed Noise: A Refined Analysis</b><br/>
        Zijian Liu<br/>
        In submission, 2025 (<span style="color:blue">Available upon request</span>)
      </li>

      <li>
        <a href="https://arxiv.org/abs/2302.06032">Near-Optimal Non-Convex Stochastic Optimization under Generalized Smoothness</a><br/>
        Zijian Liu, Srikanth Jagabathula, Zhengyuan Zhou<br/>
        Manuscript, 2023
      </li>

      <li>
        <a href="https://arxiv.org/abs/2303.12277">Stochastic Nonsmooth Convex Optimization with Heavy-Tailed Noises: HighProbability Bound, In-Expectation Rate and Initial Distance Adaptation</a><br/>
        Zijian Liu, Zhengyuan Zhou<br/>
        Manuscript, 2023
      </li>

      <li>
        <a href="https://arxiv.org/abs/2209.14853">META-STORM: Generalized Fully-Adaptive Variance Reduced SGD for Unbounded Functions</a><br/>
        Zijian Liu*, Ta Duy Nguyen*, Thien Hang Nguyen*, Alina Ene, Huy L. Nguyen<br/>
        Manuscript, 2022
      </li>
   </ol>

</body>
</html>

